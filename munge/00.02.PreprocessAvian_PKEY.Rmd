---
title: "Pre-process avian data at the PKEY level"
author: "Nicole Barker"
date: "Last run: Feb 7, 2018"
output: 
  word_document:
    reference_docx: ../styles/ReportFormat_1.docx
---
## Script Abstract

One of a series of scripts that quality-checks, corrects,  pre-processes, and merges the various tables from BAM's Avian Database. Removes duplicates. Performs some initial tests of patterns in avian data by survey method to help decide how to harmonize the data. 

This script deals with PKEY (survey instance)-level data. It does NOT add in survey-level covariates.

**SCRIPT OUTPUTS:**

1. **data.table in RData file XXXX (see cache folder on Dropbox)**
2. **csv XXXX (see output folder on Dropbox)**
3. **docx file with text and code snippets plus inline output (written directly to my repo, but ignored by Git. I'll need to copy over manually to Dropbox for sharing)**


## Background
On Nov 30, 2017, Trish provided me with the Access Database of BAM's avian data: COFI_BC_NOV30_2017.accdb. 

The BAM Database is hierarchical, with primary keys for each table compounding upon each other in the various tables. 
It's useful to look at a map to understand how the column names correspond to point count survey protocols/sampling design.  Kathy Martin's data represents a good example. 

* _PCODE_: unique code for each project
* _SITE_: Typically a cluster of point count stations
* _STN_: individual point count survey location
* _SS_: compound key comprised of PCODE:SITE:STN
* _ROUND_: If multiple visits to the same location, typically on different days.
* _PKEY_: compound key -->  PCODE:SITE:STN:YY:ROUND
* _METHOD_: The survey method (survey distance, duration); usually but not always consistent within a PCODE.
* _obs_: Identity of the survey observer.

![ ^^^ Image. BBS, Atlas (BCCA), and KMART (Kathy Martin)'s data, as an example of PCODE, SITE, and STN. Different coloured dots are from different projects (PCODEs). Kathy Martin's data (KMART) has clusters of stations in different sites (SITE), which are labelled KNIFEAFF, KNIFE7M, etc. Within (SITE) clusters are individual stations (STN). The combination of PCODE:SITE:STN makes up SS, which is a  unique ID corresponding to a given location indicated by xy coordinates](../images/KathyMartinSITEdemo.jpg)

**OFFSET-REQUIREMENTS**

* To calculate the Solymos et al 2013 style offsets (correcting for survey method, species availability, species perceptability), we need the following location-specific details: 
    * Maximum Duration (MaxDuration; MAXDUR) of survey (from method table)
    * Maximum Distance (Masdist; MAXDIS) of survey (from method table)
    * Julien date (derived from YYYY MM and DD from PKEY table)
    * Days since local spring (DSLS; derived from????
    * Time since sunrise (TSSR; derived from survey time and time zone offset (MDT_offset))


**FILES WORKED WITH IN THIS SCRIPT**

1. **BC_COFI_PKEY.txt**

This script does the following

* 
* 
* 


``` {r setup, echo=F, message=F, warning=F}
require(knitr)
opts_knit$set(root.dir = '..')
```

Source setup script
``` {r, warning=F, message=F}
source("lib/Setup_NB_WorkPC.R")
source("lib/Setup_anyComputer.R")
```

### Sampling Occasions: *BC_COFI_PKEY.csv*

``` {r load.pkey}
pkey <- data.table(read.csv(paste0(droproot, "data/BC_COFI_PKEY.csv"), header=T))
kable(rbind(head(pkey), tail(pkey)), row.names=F)
```

**Checking for Missing Data**

``` {r}
kable(as.data.frame(do.call(rbind,lapply(pkey, function(x) {sum(is.na(x))}))), caption="Number of missing values per column")
kable(as.data.frame(do.call(rbind,lapply(pkey, function(x) {length(unique(x))}))), caption="Number unique values per column")
nrow(pkey)
length(unique(pkey$PKEY))
```

**Notes**

* Some PKEYs are missing: MM, DD, HR, MIN info.
* ACTION: Long-term: Suggest Trish check into date fields to ensure complete info is available before importing up to SQL server
* ACTION: Short-term: Manually fix dates from the csv file using Excel.

### Sampling Occasions: *BC_COFI_PKEY-manuallyfixed.csv*

``` {r load.pkey}
pkey <- data.table(read.csv(paste0(droproot, "data/BC_COFI_PKEY-manuallyfixed.csv"), header=T))
kable(rbind(head(pkey), tail(pkey)), row.names=F)
```

**Checking for Missing Data**

Convert blank and 0 time and date values to NAs
``` {r}
pkey$date_FIXED[pkey$date_FIXED == ""] <- NA
pkey$time_FIXED[pkey$time_FIXED == ""] <- NA
pkey$StartTime[pkey$StartTime == ""] <- NA
pkey$mm.dd.yyy[pkey$mm.dd.yyy == ""] <- NA
pkey$HR[pkey$HR == 0] <- NA
pkey$HR[pkey$MIN == 0] <- NA
pkey$obs[pkey$obs == ""] <- NA
```

``` {r}
kable(as.data.frame(do.call(rbind,lapply(pkey, function(x) {sum(is.na(x))}))), caption="Number of missing values per column")
kable(as.data.frame(do.call(rbind,lapply(pkey, function(x) {length(unique(x))}))), caption="Number unique values per column")
nrow(pkey)
length(unique(pkey$PKEY))
```

Fix date and time field format
``` {r}
DD <- with(pkey, paste0(date_FIXED, " ", time_FIXED, ":00"))
DD <- strptime(DD, "%m/%e/%Y %H:%M:%S")
pkey$DATE <- DD
```

Add Julian day
``` {r}
pkey$JULIAN <- DD$yday # this is kept as original
pkey$JDAY <- DD$yday / 365
summary(pkey$JDAY)
hist(pkey$JULIAN)
```





















``` {r}
pkey$StartTime[pkey$StartTime == ""] <- NA #if start time is blank, make "NA"

pkey$Missing_StartTime <- NA
pkey$Missing_StartTime[!is.na(pkey$StartTime)] <- "NOT missing Sampling Date"

pkey$Missing_HR_MIN <- NA
pkey$Missing_HR_MIN[!(is.na(pkey$HR) & is.na(pkey$MIN))] <- "NOT missing Start Time"

pkey$Missing_SamplingDate <- NA
pkey$Missing_SamplingDate[!(is.na(pkey$MM) | is.na(pkey$DD))] <- "NOT missing Sampling Date"
```

**Some Checks**

``` {r}
unique(pkey[pkey$Missing_SamplingDate == "NOT missing Sampling Date",]$DD)
unique(pkey[pkey$Missing_SamplingDate == "NOT missing Sampling Date",]$MM)
```

**ERROR NOTICED** - I'm pretty sure no surveys were done in Nov and Dec, so this probably indicates a switch in MM and DD for some sites. Time to track down which ones 

``` {r}
pkey$MM.old <- pkey$MM
pkey$DD.old <- pkey$DD
pkey <- pkey[order(pkey$YYYY, pkey$SS),]

kable(rbind(head(pkey[pkey$PCODE %in% "GMSMON15", c("PCODE", "SS", "YYYY", "MM", "DD", "MM.old")], 10), tail(pkey[pkey$PCODE %in% "GMSMON15", c("PCODE", "SS", "YYYY", "MM", "DD", "MM.old")], 10)), row.names=F)

pkey$DD[pkey$PCODE %in% "GMSMON15" & pkey$YYYY == "2012"] <- pkey$MM.old[pkey$PCODE %in% "GMSMON15" & pkey$YYYY == "2012"] 

pkey$MM[pkey$PCODE %in% "GMSMON15" & pkey$YYYY == "2012"] <-
  pkey$DD.old[pkey$PCODE %in% "GMSMON15" & pkey$YYYY == "2012"]

kable(rbind(head(pkey[pkey$PCODE %in% "GMSMON15" & pkey$YYYY == "2012",c("PCODE", "SS", "YYYY", "MM", "DD")]),
            tail(pkey[pkey$PCODE %in% "GMSMON15" & pkey$YYYY == "2012",c("PCODE", "SS", "YYYY", "MM", "DD")])), row.names=F)
unique(pkey$MM)
```

Alright, so that fixed the obvious date issues. Let's look at some other potential data problems. 

``` {r}
pkey$DATE <- as.Date(paste(pkey$YYYY, pkey$MM, pkey$DD, sep="/"))
```

**Notes**

* The pkey table has `r nrow(pkey)` rows covering `r length(unique(pkey$SS))` SS from `r length(unique(pkey$SITE))` sites over `r length(unique(pkey$PCODE))` projects. This corresponds to `r length(unique(pkey$PKEY))` unique PKEYS
* The earliest point count was done in `r min(pkey$YYYY)`.
* Sometimes the addition of WSI data added projects we already had from the Atlas. So we need to look for duplicated locations and years to remove those duplicates.
